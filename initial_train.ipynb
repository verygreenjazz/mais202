{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1g4gCFkO9-70Jj_r7WwqhU-wtm-cqWpgM",
      "authorship_tag": "ABX9TyO3ZOCPyf/9Vm41O7qb+thi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CM-axLZNMl0X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "\n",
        "\n",
        "# Load a pre-trained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the last layer to match waste labels (30 labels)\n",
        "model.fc = nn.Linear(model.fc.in_features, 30)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Transform images to 224x224 and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "# Load the Recyclable and House Waste dataset\n",
        "\n",
        "class WasteDataset():\n",
        "    def __init__(self, split, root_dir=\"/content/drive/MyDrive/MAIS202/dataset/images/images\", transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(os.listdir(root_dir))\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for i, class_name in enumerate(self.classes):\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            for subfolder in ['default', 'real_world']:\n",
        "                subfolder_dir = os.path.join(class_dir, subfolder)\n",
        "                image_names = os.listdir(subfolder_dir)\n",
        "                random.shuffle(image_names)\n",
        "\n",
        "                if split == 'train':\n",
        "                    image_names = image_names[:int(0.6 * len(image_names))]\n",
        "                elif split == 'val':\n",
        "                    image_names = image_names[int(0.6 * len(image_names)):int(0.8 * len(image_names))]\n",
        "                else:  # split == 'test'\n",
        "                    image_names = image_names[int(0.8 * len(image_names)):]\n",
        "\n",
        "                for image_name in image_names:\n",
        "                    self.image_paths.append(os.path.join(subfolder_dir, image_name))\n",
        "                    self.labels.append(i)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        data = {\n",
        "            \"image\":image,\n",
        "            \"label\":label,\n",
        "        }\n",
        "        return data\n",
        "\n",
        "\n",
        "# Prepare datasets and dataloaders\n",
        "\n",
        "train_pil_transform = v2.Compose([\n",
        "        v2.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.2),\n",
        "        v2.RandomAffine(degrees=5, translate=(0.1, 0.1),scale=(0.8,1.3),\n",
        "                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
        "        v2.Resize(size=(256, 256)),\n",
        "        v2.GaussianBlur(kernel_size=(7, 13), sigma=(0.1, 0.2)),\n",
        "        v2.PILToTensor(),\n",
        "        v2.ToDtype(torch.float32),\n",
        "        v2.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))\n",
        "    ])\n",
        "\n",
        "val_pil_transform = v2.Compose([\n",
        "    v2.Resize(size=(256, 256)),\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_pil_transform = v2.Compose([\n",
        "    v2.Resize(size=(256, 256)),\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "data_transforms = {\n",
        "    \"train\":train_pil_transform,\n",
        "    \"val\":val_pil_transform,\n",
        "    \"test\":test_pil_transform,\n",
        "}\n",
        "\n",
        "train_dataset = WasteDataset(split=\"train\", transform=data_transforms[\"train\"])\n",
        "val_dataset = WasteDataset(split=\"val\", transform=data_transforms[\"val\"])\n",
        "test_dataset = WasteDataset(split=\"test\", transform=data_transforms[\"test\"])\n",
        "\n",
        "\n",
        "\n",
        "# Load the Recyclable and House Waste dataset\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "validateloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler to adjust the learning rate\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "IAIjUjZICJpy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning\n",
        "num_epochs = 5\n",
        "# the number of epochs is set to 5 for the initial train\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for idx, data in enumerate(trainloader): # each entry in trainloader (each idx) is a batch of 64 images and their labels\n",
        "        images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()* images.size(0)\n",
        "\n",
        "    # Step the scheduler after each epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print('Fine-tuning complete')\n",
        "# took 5 hours to run!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNSWQs6-7ZRp",
        "outputId": "a07e81ce-614a-4eaa-8dde-b7b65378a2b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 102.0208\n",
            "Epoch [2/5], Loss: 68.6975\n",
            "Epoch [3/5], Loss: 55.4733\n",
            "Epoch [4/5], Loss: 47.3633\n",
            "Epoch [5/5], Loss: 40.0380\n",
            "Fine-tuning complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/MAIS202/initial_train_resnet18_mnist.pth\"\n",
        "\n",
        "torch.save(model.state_dict(), save_path) # in the future, save after each epoch so that training on more than 5 epochs can be done stepwise\n",
        "print('Model saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTD5Y4TUxPhz",
        "outputId": "c35b8213-7aa9-4f60-a4d8-cdaa45553626"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 30)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/MAIS202/initial_train_resnet18_mnist.pth\"))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "correctSort = 0\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, data in enumerate(testloader): # each entry in testloader (each idx) is a batch of 64 images and their labels\n",
        "        images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the fine-tuned model on the test images: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFgmQLx334Uf",
        "outputId": "03ee1f50-98f6-4bb5-987b-dd9bb1d842e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the fine-tuned model on the test images: 77.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch = next(iter(testloader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    sample_outputs = model(sample_batch[\"image\"].to(device))\n",
        "    print(torch.argmax(sample_outputs, dim=1))\n",
        "    print(sample_batch[\"label\"].to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuQIyQo3BMZw",
        "outputId": "f89346ea-9cf3-4c14-c4ff-ecd6f0cb93da"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12, 20, 13, 13, 14, 20,  8, 17, 22, 11,  0, 13,  3,  4,  9, 10, 13, 16,\n",
            "        25,  6, 12, 24, 23,  8,  4,  5,  5,  3, 11, 10, 22, 27, 17, 25, 23, 17,\n",
            "        17,  6,  4, 18, 22,  6, 15, 26,  4,  6, 22, 23, 25,  5,  1, 27, 15,  4,\n",
            "        29, 24,  2, 27, 26, 12,  3, 29, 14,  9])\n",
            "tensor([12,  7, 13, 22, 24, 20,  8, 17, 22, 11,  0, 15, 15,  4,  9, 10, 13, 16,\n",
            "        25,  6, 12, 22, 20,  8, 15,  5,  5,  3, 11, 10, 22, 27, 17, 25, 26, 17,\n",
            "        17,  6,  4, 18, 22,  6, 15, 26,  4,  6, 22, 23, 25, 22, 11, 27, 16,  4,\n",
            "        14, 24,  2, 27,  1, 12,  4, 29, 14,  9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_outputs.min(), sample_outputs.max()) # maybe fixing the logit values will reduce the loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGy4xJBdHYFw",
        "outputId": "da834aee-093e-41e8-958d-8d375985bf11"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-16.1039) tensor(22.6542)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code adapted from this tutorial: https://dev.to/santoshpremi/fine-tuning-a-pre-trained-model-in-pytorch-a-step-by-step-guide-for-beginners-4p6l"
      ],
      "metadata": {
        "id": "iRG4E1XqaEsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}